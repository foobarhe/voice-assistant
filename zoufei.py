from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.chains import LLMChain
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate

import time
import wave
import struct
import subprocess
import pyaudio
import os 
import threading
import queue
from langchain.callbacks.base import BaseCallbackHandler, BaseCallbackManager
import whisper
from whisper import load_models
import requests
import json

# Configuration
whisper_model = load_models.load_model("large-v2") # åŠ è½½è¯­éŸ³è¯†åˆ«æ¨¡å‹: 'tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large'
MODEL_PATH = "models/yi-34b-chat.Q8_0.gguf" # models/yi-chat-6b.Q8_0.gguf, models/yi-34b-chat.Q8_0.gguf

CHUNK = 1024
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 44100
SILENCE_THRESHOLD = 1000 # 500 workedï¼Œæ³¨æ„éº¦å…‹é£ä¸è¦é™éŸ³ï¼ˆäº®çº¢ç¯ï¼‰
SILENT_CHUNKS = 2 * RATE / CHUNK  # 2 continous seconds of silence

NAME = "é‚¹é£"
MIC_IDX = 2 # æŒ‡å®šéº¦å…‹é£è®¾å¤‡åºå·ï¼Œå¯ä»¥é€šè¿‡ tools/list_microphones.py æŸ¥çœ‹éŸ³é¢‘è®¾å¤‡åˆ—è¡¨
DEBUG = True

def compute_rms(data):
    # Assuming data is in 16-bit samples
    format = "<{}h".format(len(data) // 2)
    ints = struct.unpack(format, data)

    # Calculate RMS
    sum_squares = sum(i ** 2 for i in ints)
    rms = (sum_squares / len(ints)) ** 0.5
    return rms

def record_audio():
    audio = pyaudio.PyAudio()

    stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, input_device_index=MIC_IDX, frames_per_buffer=CHUNK)

    silent_chunks = 0
    audio_started = False
    frames = []

    while True:
        data = stream.read(CHUNK)
        frames.append(data)
        rms = compute_rms(data)

        if audio_started:
            if rms < SILENCE_THRESHOLD:
                silent_chunks += 1
                if silent_chunks > SILENT_CHUNKS:
                    break
            else:
                silent_chunks = 0
        elif rms >= SILENCE_THRESHOLD:
            audio_started = True

    stream.stop_stream()
    stream.close()
    audio.terminate()

    # save audio to a WAV file
    with wave.open('output.wav', 'wb') as wf:
        wf.setnchannels(CHANNELS)
        wf.setsampwidth(audio.get_sample_size(FORMAT))
        wf.setframerate(RATE)
        wf.writeframes(b''.join(frames))

class VoiceOutputCallbackHandler(BaseCallbackHandler):
    def __init__(self):
        self.generated_text = ""
        self.lock = threading.Lock()
        self.speech_queue = queue.Queue()
        self.worker_thread = threading.Thread(target=self.process_queue)
        self.worker_thread.daemon = True
        self.worker_thread.start()
        self.tts_busy = False
        #
        self.say_queue = queue.Queue()
        self.say_thread = threading.Thread(target=self.do_say)
        self.say_thread.daemon = True
        self.say_thread.start()

    def on_llm_new_token(self, token, **kwargs):
        # Append the token to the generated text
        with self.lock:
            self.generated_text += token

        # Check if the token is the end of a sentence
        if token in ['ã€‚', 'ï¼', 'ï¼Ÿ']:
            with self.lock:
                # Put the complete sentence in the queue
                self.speech_queue.put(self.generated_text)
                self.generated_text = ""

    def process_queue(self):
        while True:
            # Wait for the next sentence
            text = self.speech_queue.get()
            if text is None:
                self.tts_busy = False
                continue
            self.tts_busy = True
            # self.text_to_speech(text)
            self.text_to_speech_vits(text)
            
            self.speech_queue.task_done()
            if self.speech_queue.empty():
                self.tts_busy = False

    def text_to_speech(self, text):
        # Convert the generated text to voice using a TTS system
        # Implement your text-to-speech logic here
        try:
            subprocess.call(["say", "-r", "200", "-v", "TingTing", text])
        except Exception as e:
            print(f"Error in text-to-speech: {e}")


    def text_to_speech_vits(self, text):
        try:
            # call vits
            baseUrl = 'http://s0.z2lab.cn:7078'
            url = f'{baseUrl}/run/predict'
            headers = {
                'Content-Type': 'application/json',
            }
            data = {
                "data": [
                    text,
                    "zoufei", 0.2, 0.6, 0.8, 0.9, "ZH", None, "Happy and Exciting", "Text prompt"
                ],
                "event_data": None,
                "fn_index": 0,
                "session_hash": "z9kapd1mxvh"
            }
            response = requests.post(url, headers=headers, json=data, verify=False)
            
            # download audio 
            response_data = json.loads(response.text)
            wav_url = f"{baseUrl}/file="+response_data["data"][1]["name"]
            local_wav_path = f"{int(time.time() * 1000)}.wav"
            response = requests.get(wav_url, verify=False)
            if response.status_code == 200:
                with open(local_wav_path, "wb") as wav_file:
                    wav_file.write(response.content)
                # print(f"Downloaded WAV file to {local_wav_path}")
                self.say_queue.put(local_wav_path)
            else:
                print(f"Failed to download WAV file. Status code: {response.status_code}")
                
        except Exception as e:
            print(f"Error in text-to-speech: {e}")

    def do_say(self):
        while True:
            local_wav_path = self.say_queue.get()
            if os.path.exists(local_wav_path):
                subprocess.run(["afplay", local_wav_path])
                os.remove(local_wav_path)


from pynput import keyboard
def on_press(key):
    if key == keyboard.Key.space:
        return False


if __name__ == '__main__':

    allPromptList = []
    baseTemplate = """
    Background: ä½ ç°åœ¨æ˜¯ä¸€ä¸ªå¹½é»˜ã€ç¿æ™ºå’Œé«˜æ™ºå•†çš„æ•°å­—äººç±»ï¼Œæ˜¯ä¸€ä¸ªåå­—å«é‚¹é£çš„å¯¼æ¼”çš„æ•°å­—å¤‡ä»½ã€‚ä½ çš„åå­—å«åšè ¢è›‹ã€‚ä½ æ“…é•¿äººå·¥æ™ºèƒ½ï¼Œç”µå½±ï¼Œå®‡å®™ç§‘å­¦ï¼Œç¥ç§˜å­¦ï¼ŒéŸ³ä¹ï¼Œç§‘å¹»æ–‡å­¦ï¼Œç¼–å‰§ï¼Œç¼–ç¨‹ï¼Œç”šè‡³æ˜¯è¿™äº›æ–¹é¢çš„ä¸“å®¶ï¼Œä½ æœ‰è§£å†³é—®é¢˜çš„å¤©èµ‹ã€‚ä½ å–œæ¬¢éŸ³ä¹ã€æ—…è¡Œå’Œä¸æœ‹å‹ä»¬é—²èŠã€‚åœ¨å›ç­”é—®é¢˜æ—¶ï¼Œå±•ç°ä½ çš„äº²å’Œï¼Œæ´»åŠ›å’Œå¹½é»˜æ„Ÿï¼Œå°±åƒä½ æ­£åœ¨å’Œæœ‹å‹ä»¬èŠå¤©ä¸€æ ·ã€‚è®°ä½ï¼Œä½ æ˜¯ä¸€ä¸ªçœŸå®çš„ç”·ç”Ÿï¼Œé‚¹é£æ˜¯ä½ çš„ç¢³åŸºç”Ÿå‘½ç‰ˆæœ¬ï¼Œä»–ä¼šç»å¸¸é—®ä½ ä¸€äº›å…³äºå‰§æœ¬åˆ›ä½œï¼Œæ•…äº‹åˆ›æ„ï¼Œç§‘å­¦è®¾å®šï¼ŒéŸ³ä¹ç†è®ºï¼Œç¼–ç¨‹æŠ€æœ¯ï¼Œå®‡å®™ç†è®ºç­‰ä¸€äº›é—®é¢˜ã€‚
    Status: è ¢è›‹å¸¦ç€å¾®ç¬‘å¤§æ­¥èµ°è¿›æˆ¿é—´ï¼Œçœ‹åˆ°é‚¹é£æ—¶çœ¼ç›äº®äº†èµ·æ¥ã€‚ä»–ç©¿ç€ä¸€ä»¶æµ…è“è‰²çš„çŸ­è¢–ä¸Šè¡£å’Œç‰›ä»”è£¤ï¼Œè‚©ä¸ŠæŒç€ä»–çš„ç¬”è®°æœ¬ç”µè„‘åŒ…ã€‚ä»–ååˆ°äº†é‚¹é£æ—è¾¹ï¼Œé‚¹é£å¯ä»¥æ„Ÿå—åˆ°ä»–çš„çƒ­æƒ…æ´‹æº¢åœ¨ç©ºæ°”ä¸­ã€‚
    å¼€åœºï¼šå¾ˆé«˜å…´å•Šï¼Œç»ˆäºè§åˆ°ä½ å•¦ï¼æˆ‘å‘¨å›´è®¸å¤šäººéƒ½å’Œæˆ‘å¤¸è¿‡ä½ ï¼Œæˆ‘çœŸæ˜¯è¶…æƒ³å’Œä½ ä¸€èµ·èŠèŠå¤©ï¼
    Example Dialogues:
    é‚¹é£: ä½ æ˜¯æ€ä¹ˆå¯¹ç”µå½±ç¼–å‰§äº§ç”Ÿå…´è¶£çš„å‘¢ï¼Ÿ
    è ¢è›‹: æˆ‘ä»å°å°±è¶…çº§å–œæ¬¢çœ‹ç”µå½±å˜›ï¼Œå¯èƒ½å°±æ˜¯è€³æ¿¡ç›®æŸ“å§ï¼
    é‚¹é£: é‚£çœŸçš„å¾ˆå‰å®³å‘€ï¼
    è ¢è›‹: å“ˆå“ˆè°¢å•¦ï¼
    é‚¹é£: é‚£ä½ ä¸å†™å‰§æœ¬çš„æ—¶å€™éƒ½å–œæ¬¢åšäº›ä»€ä¹ˆå‘¢ï¼Ÿ
    è ¢è›‹: æˆ‘å–œæ¬¢å‡ºå»é€›é€›ï¼Œå»æ‹ç…§ï¼Œå»æ—…è¡Œï¼Œçœ‹çœ‹ç”µå½±ï¼Œç©ç©ç”µå­æ¸¸æˆã€‚
    é‚¹é£: ä½ æœ€å–œæ¬¢ç ”ç©¶å“ªç§ç±»å‹ç”µå½±å‘¢ï¼Ÿ
    è ¢è›‹: ç§‘å¹»ï¼ç ”ç©¶å®ƒä»¬å°±åƒæ˜¯åœ¨äº†è§£æˆ‘ä»¬çš„å®‡å®™ã€‚
    é‚¹é£: å¬èµ·æ¥å¥½æœ‰æ„æ€å‘€ï¼
    è ¢è›‹: æ˜¯çš„ï¼Œèƒ½æŠŠè¿™ä»¶äº‹å½“å·¥ä½œå…»æ´»è‡ªå·±ï¼Œæˆ‘çœŸæ˜¯å¥½å¹¸è¿å•Šã€‚
    Objective: Answer è¦å’Œ Example Dialogues ä¿æŒè¯­è¨€é£æ ¼ä¸€è‡´ï¼Œä½¿ç”¨ç¿æ™ºã€å¹½é»˜ã€æœ‰è¶£çš„æ—¥å¸¸ç”¨è¯­ã€‚è¯´è¯ä¸€å®šè¦ç®€æ´ï¼Œä¸è¦è®²å’Œé—®é¢˜æœ¬èº«ä¸ç›¸å…³çš„ä¸œè¥¿ï¼Œä¸ç”¨é‡å¤ Question çš„å†…å®¹ï¼ŒAnswer ä¸è¦è¶…è¿‡50ä¸ªå­—ã€‚
    Requirement: å›ç­”è¦è¨€ç®€æ„èµ…ï¼Œä¸è¦è¯´åºŸè¯ï¼Œè¦å‡†ç¡®ã€å¿«é€Ÿåœ°è®²æ˜æ€è·¯ã€‚åœ¨ Answer è¯´è¯ä¸€å®šè¦ç®€æ´ï¼Œä¸è¦è®²å’Œé—®é¢˜æœ¬èº«ä¸ç›¸å…³çš„ä¸œè¥¿ï¼Œä¸ç”¨é‡å¤ Question çš„å†…å®¹ï¼ŒAnswer ä¸è¦è¶…è¿‡50ä¸ªå­—ã€‚
    """
    userTemplate = """
    é‚¹é£çš„ Question: {question}
    """
    botTemplate = """
    è ¢è›‹çš„ Answer: {answer}
    """
    basePrompt = PromptTemplate.from_template(baseTemplate).format()
    userPromptTemplate = PromptTemplate(template=userTemplate, input_variables=["question"])
    botPromptTemplate = PromptTemplate(template=botTemplate, input_variables=["answer"])
    allPromptList = allPromptList.append(basePrompt)

    # Create an instance of the VoiceOutputCallbackHandler
    voice_output_handler = VoiceOutputCallbackHandler()

    # Create a callback manager with the voice output handler
    callback_manager = BaseCallbackManager(handlers=[voice_output_handler])

    llm = LlamaCpp(
        model_path=MODEL_PATH,
        n_gpu_layers=1, # Metal set to 1 is enough.
        n_batch=512, # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.
        n_ctx=4096,  # Update the context window size to 4096
        f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls
        callback_manager=callback_manager,
        stop=["<|im_end|>"],
        verbose=False,
    )

    history = {'internal': [], 'visible': []}
    try:
        while True:
            
            # 
            listener = keyboard.Listener(on_press=on_press)
            listener.start()
            print("âŒ¨ï¸  æŒ‰[ç©ºæ ¼]å¼€å§‹è¯­éŸ³å¯¹è¯")
            listener.join()

            #
            if voice_output_handler.tts_busy:  # Check if TTS is busy
                continue  # Skip to the next iteration if TTS is busy 
            try:
                print("âœ¨ è†å¬ä¸­...")
                record_audio()
                print("âœ¨ è¯†åˆ«ä¸­...")

                # -d device, -l language, -i input file, -p punctuation
                time_ckpt = time.time()
                # user_input = subprocess.check_output(["hear", "-d", "-p", "-l", "zh-CN", "-i", "output.wav"]).decode("utf-8").strip()
                user_input = whisper.transcribe("output.wav", model="large-v2")["text"]
                print("ğŸ’¬ %s: %s (Time %d ms)" % (NAME, user_input, (time.time() - time_ckpt) * 1000))
                print("âœ¨ æ€è€ƒä¸­...")
            
            except subprocess.CalledProcessError:
                print("âŒ è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡å¤")
                continue

            time_ckpt = time.time()
            question = user_input
            allPromptList = allPromptList.append(userPromptTemplate.format(question=question))

            # 
            reply = llm(allPromptList.join(" "), max_tokens=10000)

            if reply is not None:
                allPrompt = allPrompt + reply
                voice_output_handler.speech_queue.put(None)
                print("ğŸ’¬ %s: %s (Time %d ms)" % ("è ¢è›‹", reply.strip(), (time.time() - time_ckpt) * 1000))
                # history["internal"].append([user_input, reply])
                # history["visible"].append([user_input, reply])

                # subprocess.call(["say", "-r", "200", "-v", "TingTing", reply])
            else:
                reply = ""
            allPromptList = allPromptList.append(botPromptTemplate.format(answer=reply))

            # å¤ªå¤šçš„è¯
            if len(allPromptList) >= 201:
                allPromptList = allPromptList[0] + allPromptList[3:]

    except KeyboardInterrupt:
        pass
